{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We'll start by an introduction to Keras.\n",
    "\n",
    "Most of the examples are from [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Keras and MNIST\n",
    "Let us first build an introductory model for the MNIST dataset.  \n",
    "More on MNIST can be found here: https://www.kaggle.com/hojjatk/mnist-dataset?msclkid=a59b1a61bffd11ec953ecf3f2a143919  \n",
    "More on Layers API can be found here: https://keras.io/api/layers/?msclkid=0799043ac07911ec832b34d2444c3574  \n",
    "On layers types, regularizers, initializers (beyond the Keras Layerd Documentation): https://www.tutorialspoint.com/keras/keras_layers.htm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the MNIST dataset in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], shape=(10000,), dtype=uint8)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So network architecture for MNIST.  \n",
    "For example: https://github.com/JHP4911/htMultiple-MLP-Architectures-on-MNIST-database-using-Keras?msclkid=b0b3576ec08a11ecb731533187ed8284  \n",
    "Or in the future even: https://keras.io/examples/vision/mnist_convnet/?msclkid=b0b3b888c08a11eca260b4525969c7f7  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions introduce **non-linearity**.  \n",
    "Without them, deep networks reduce to linear transformations.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "[0, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Hidden layers in most feedforward and convolutional networks  \n",
    "\n",
    "Why useful: \n",
    "- Simple and fast  \n",
    "- Helps mitigate vanishing gradients  \n",
    "- Sparse activations  \n",
    "\n",
    "Limitation: \n",
    "- Can cause “dying ReLU” (neurons stuck at 0)\n",
    "\n",
    "**Leaky ReLU**\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Hidden layers when ReLU leads to many inactive neurons  \n",
    "\n",
    "Why useful:\n",
    "- Keeps small gradient for negative values  \n",
    "- More stable than standard ReLU  \n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(0, 1)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Output layer in binary classification  \n",
    "\n",
    "Why useful:\n",
    "- Interpretable as probability  \n",
    "\n",
    "Limitation:\n",
    "- Vanishing gradients  \n",
    "- Not suitable for deep hidden layers  \n",
    "\n",
    "**Tanh**\n",
    "\n",
    "$$\n",
    "f(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-1, 1)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Some recurrent neural networks (RNNs)  \n",
    "- Smaller or older architectures  \n",
    "\n",
    "Why useful: \n",
    "- Zero-centered output  \n",
    "\n",
    "Limitation:\n",
    "- Still suffers from vanishing gradients  \n",
    "\n",
    "**Softmax**\n",
    "\n",
    "$$\n",
    "f(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range (each component):\n",
    "$$\n",
    "(0, 1)\n",
    "$$\n",
    "\n",
    "Additionally:\n",
    "$$\n",
    "\\sum_i f(x_i) = 1\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Output layer in multi-class classification  \n",
    "\n",
    "Why useful:\n",
    "- Produces probability distribution over classes  \n",
    "- Works naturally with cross-entropy loss  \n",
    "\n",
    "**GELU (Gaussian Error Linear Unit)**\n",
    "\n",
    "$$\n",
    "f(x) = x \\Phi(x)\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Transformer architectures (e.g., modern NLP models)  \n",
    "\n",
    "Why useful: \n",
    "- Smooth activation  \n",
    "- Empirically strong performance in large models  \n",
    "\n",
    "**Swish**\n",
    "\n",
    "$$\n",
    "f(x) = x \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:  \n",
    "- Modern architectures where smooth activations are preferred  \n",
    "\n",
    "Why useful:\n",
    "- Good gradient flow  \n",
    "- Sometimes outperforms ReLU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are used to evaluate model performance.  \n",
    "They do not guide training — they only measure the results.\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$\n",
    "\n",
    "Use for: balanced classification problems  \n",
    "Why useful: simple and interpretable  \n",
    "Limitation: misleading with imbalanced datasets  \n",
    "\n",
    "**Precision**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Use for: when false positives are costly  \n",
    "Why useful: measures correctness of positive predictions  \n",
    "\n",
    "**Recall**\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Use for: when false negatives are costly  \n",
    "Why useful: measures ability to detect positives  \n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: penalizes large errors strongly  \n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N}|y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: more robust to outliers than MSE  \n",
    "\n",
    "**ROC-AUC**\n",
    "\n",
    "Area under the ROC curve.\n",
    "\n",
    "Use for: binary classification  \n",
    "Why useful: measures separability independent of threshold  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions are minimized during training.  \n",
    "They define the optimization objective.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: smooth and differentiable  \n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N}|y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: less sensitive to outliers  \n",
    "\n",
    "**Binary Cross-Entropy (BCE)**\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\left[\n",
    "y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Use for: binary classification  \n",
    "Why useful: works naturally with sigmoid outputs  \n",
    "\n",
    "**Categorical Cross-Entropy**\n",
    "\n",
    "$$\n",
    "L = - \\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Use for: multi-class classification  \n",
    "Why useful: works with softmax outputs  \n",
    "\n",
    "**Hinge Loss**\n",
    "\n",
    "$$\n",
    "L = \\max(0, 1 - y \\hat{y})\n",
    "$$\n",
    "\n",
    "Use for: margin-based classification (SVM-style models)  \n",
    "Why useful: encourages separation margin  \n",
    "\n",
    "**KL Divergence**\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\| Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "Use for: comparing probability distributions  \n",
    "Why useful: common in generative models and VAEs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adamax\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential, built=False>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing the image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = test_images[2].reshape(28, 28)\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**\"Fitting\" the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9016 - loss: 0.3637\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9478 - loss: 0.1883\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9604 - loss: 0.1402\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9691 - loss: 0.1111\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9747 - loss: 0.0909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1458423fe00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the model to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_digits = test_images[0:10]\n",
    "predictions = model(test_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([4.3052819e-06, 5.7998417e-08, 5.6557365e-05, 1.2214294e-03,\n",
       "       3.5749782e-08, 5.6995709e-06, 2.1951137e-10, 9.9866879e-01,\n",
       "       4.7686499e-06, 3.8259761e-05], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9903753399848938>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Evaluating the model on new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0986\n",
      "test_acc: 0.97079998254776\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Reimplementing our first example from scratch in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Keras vs. Tensorflow](../img/keras-tensorflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A simple Dense class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "\n",
    "        b_shape = (output_size,)\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A simple Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "           x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "       weights = []\n",
    "       for layer in self.layers:\n",
    "           weights += layer.weights\n",
    "       return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Running one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 5.02\n",
      "loss at batch 100: 2.24\n",
      "loss at batch 200: 2.21\n",
      "loss at batch 300: 2.11\n",
      "loss at batch 400: 2.22\n",
      "Epoch 1\n",
      "loss at batch 0: 1.92\n",
      "loss at batch 100: 1.89\n",
      "loss at batch 200: 1.84\n",
      "loss at batch 300: 1.73\n",
      "loss at batch 400: 1.84\n",
      "Epoch 2\n",
      "loss at batch 0: 1.60\n",
      "loss at batch 100: 1.59\n",
      "loss at batch 200: 1.52\n",
      "loss at batch 300: 1.45\n",
      "loss at batch 400: 1.52\n",
      "Epoch 3\n",
      "loss at batch 0: 1.34\n",
      "loss at batch 100: 1.35\n",
      "loss at batch 200: 1.26\n",
      "loss at batch 300: 1.23\n",
      "loss at batch 400: 1.28\n",
      "Epoch 4\n",
      "loss at batch 0: 1.14\n",
      "loss at batch 100: 1.16\n",
      "loss at batch 200: 1.06\n",
      "loss at batch 300: 1.07\n",
      "loss at batch 400: 1.11\n",
      "Epoch 5\n",
      "loss at batch 0: 0.99\n",
      "loss at batch 100: 1.02\n",
      "loss at batch 200: 0.92\n",
      "loss at batch 300: 0.95\n",
      "loss at batch 400: 0.99\n",
      "Epoch 6\n",
      "loss at batch 0: 0.88\n",
      "loss at batch 100: 0.92\n",
      "loss at batch 200: 0.82\n",
      "loss at batch 300: 0.86\n",
      "loss at batch 400: 0.90\n",
      "Epoch 7\n",
      "loss at batch 0: 0.80\n",
      "loss at batch 100: 0.83\n",
      "loss at batch 200: 0.74\n",
      "loss at batch 300: 0.79\n",
      "loss at batch 400: 0.83\n",
      "Epoch 8\n",
      "loss at batch 0: 0.73\n",
      "loss at batch 100: 0.76\n",
      "loss at batch 200: 0.68\n",
      "loss at batch 300: 0.73\n",
      "loss at batch 400: 0.78\n",
      "Epoch 9\n",
      "loss at batch 0: 0.68\n",
      "loss at batch 100: 0.71\n",
      "loss at batch 200: 0.63\n",
      "loss at batch 300: 0.68\n",
      "loss at batch 400: 0.73\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "keras-intro.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
