{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We'll start by an introduction to Keras.\n",
    "\n",
    "Most of the examples are from [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Keras and MNIST\n",
    "Let us first build an introductory model for the MNIST dataset.  \n",
    "More on MNIST can be found here: https://www.kaggle.com/hojjatk/mnist-dataset?msclkid=a59b1a61bffd11ec953ecf3f2a143919  \n",
    "More on Layers API can be found here: https://keras.io/api/layers/?msclkid=0799043ac07911ec832b34d2444c3574  \n",
    "On layers types, regularizers, initializers (beyond the Keras Layerd Documentation): https://www.tutorialspoint.com/keras/keras_layers.htm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the MNIST dataset in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(5)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], shape=(10000,), dtype=uint8)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So network architecture for MNIST.  \n",
    "For example: https://github.com/JHP4911/htMultiple-MLP-Architectures-on-MNIST-database-using-Keras?msclkid=b0b3576ec08a11ecb731533187ed8284  \n",
    "Or in the future even: https://keras.io/examples/vision/mnist_convnet/?msclkid=b0b3b888c08a11eca260b4525969c7f7  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions introduce **non-linearity**.  \n",
    "Without them, deep networks reduce to linear transformations.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "[0, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Hidden layers in most feedforward and convolutional networks  \n",
    "\n",
    "Why useful: \n",
    "- Simple and fast  \n",
    "- Helps mitigate vanishing gradients  \n",
    "- Sparse activations  \n",
    "\n",
    "Limitation: \n",
    "- Can cause “dying ReLU” (neurons stuck at 0)\n",
    "\n",
    "**Leaky ReLU**\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Hidden layers when ReLU leads to many inactive neurons  \n",
    "\n",
    "Why useful:\n",
    "- Keeps small gradient for negative values  \n",
    "- More stable than standard ReLU  \n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(0, 1)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Output layer in binary classification  \n",
    "\n",
    "Why useful:\n",
    "- Interpretable as probability  \n",
    "\n",
    "Limitation:\n",
    "- Vanishing gradients  \n",
    "- Not suitable for deep hidden layers  \n",
    "\n",
    "**Tanh**\n",
    "\n",
    "$$\n",
    "f(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-1, 1)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Some recurrent neural networks (RNNs)  \n",
    "- Smaller or older architectures  \n",
    "\n",
    "Why useful: \n",
    "- Zero-centered output  \n",
    "\n",
    "Limitation:\n",
    "- Still suffers from vanishing gradients  \n",
    "\n",
    "**Softmax**\n",
    "\n",
    "$$\n",
    "f(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range (each component):\n",
    "$$\n",
    "(0, 1)\n",
    "$$\n",
    "\n",
    "Additionally:\n",
    "$$\n",
    "\\sum_i f(x_i) = 1\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Output layer in multi-class classification  \n",
    "\n",
    "Why useful:\n",
    "- Produces probability distribution over classes  \n",
    "- Works naturally with cross-entropy loss  \n",
    "\n",
    "**GELU (Gaussian Error Linear Unit)**\n",
    "\n",
    "$$\n",
    "f(x) = x \\Phi(x)\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:\n",
    "- Transformer architectures (e.g., modern NLP models)  \n",
    "\n",
    "Why useful: \n",
    "- Smooth activation  \n",
    "- Empirically strong performance in large models  \n",
    "\n",
    "**Swish**\n",
    "\n",
    "$$\n",
    "f(x) = x \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Range:\n",
    "$$\n",
    "(-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Use for:  \n",
    "- Modern architectures where smooth activations are preferred  \n",
    "\n",
    "Why useful:\n",
    "- Good gradient flow  \n",
    "- Sometimes outperforms ReLU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are used to evaluate model performance.  \n",
    "They do not guide training — they only measure the results.\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$\n",
    "\n",
    "Use for: balanced classification problems  \n",
    "Why useful: simple and interpretable  \n",
    "Limitation: misleading with imbalanced datasets  \n",
    "\n",
    "**Precision**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Use for: when false positives are costly  \n",
    "Why useful: measures correctness of positive predictions  \n",
    "\n",
    "**Recall**\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Use for: when false negatives are costly  \n",
    "Why useful: measures ability to detect positives  \n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: penalizes large errors strongly  \n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N}|y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: more robust to outliers than MSE  \n",
    "\n",
    "**ROC-AUC**\n",
    "\n",
    "Area under the ROC curve.\n",
    "\n",
    "Use for: binary classification  \n",
    "Why useful: measures separability independent of threshold  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions are minimized during training.  \n",
    "They define the optimization objective.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: smooth and differentiable  \n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N}|y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Use for: regression  \n",
    "Why useful: less sensitive to outliers  \n",
    "\n",
    "**Binary Cross-Entropy (BCE)**\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\left[\n",
    "y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Use for: binary classification  \n",
    "Why useful: works naturally with sigmoid outputs  \n",
    "\n",
    "**Categorical Cross-Entropy**\n",
    "\n",
    "$$\n",
    "L = - \\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Use for: multi-class classification  \n",
    "Why useful: works with softmax outputs  \n",
    "\n",
    "**Hinge Loss**\n",
    "\n",
    "$$\n",
    "L = \\max(0, 1 - y \\hat{y})\n",
    "$$\n",
    "\n",
    "Use for: margin-based classification (SVM-style models)  \n",
    "Why useful: encourages separation margin  \n",
    "\n",
    "**KL Divergence**\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\| Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "Use for: comparing probability distributions  \n",
    "Why useful: common in generative models and VAEs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adamax\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_5, built=False>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing the image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = test_images[0].reshape(28, 28)\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**\"Fitting\" the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6312 - loss: 1.8747\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7825 - loss: 1.0182\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8372 - loss: 0.6780\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8647 - loss: 0.5353\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.8792 - loss: 0.4626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c8fe611bf0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the model to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_digits = test_images[0:10]\n",
    "predictions = model(test_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([1.61205448e-04, 5.84136160e-06, 6.14488890e-05, 6.98857184e-04,\n",
       "       1.05951025e-04, 2.18515459e-04, 1.07934943e-06, 9.90375340e-01,\n",
       "       9.04471235e-05, 8.28124210e-03], dtype=float32)>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9903753399848938>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(7)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Evaluating the model on new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8901 - loss: 0.4222\n",
      "test_acc: 0.8901000022888184\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Reimplementing our first example from scratch in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Keras vs. Tensorflow](../img/keras-tensorflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A simple Dense class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "\n",
    "        b_shape = (output_size,)\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A simple Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "           x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "       weights = []\n",
    "       for layer in self.layers:\n",
    "           weights += layer.weights\n",
    "       return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Running one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 5.02\n",
      "loss at batch 100: 2.24\n",
      "loss at batch 200: 2.21\n",
      "loss at batch 300: 2.11\n",
      "loss at batch 400: 2.22\n",
      "Epoch 1\n",
      "loss at batch 0: 1.92\n",
      "loss at batch 100: 1.89\n",
      "loss at batch 200: 1.84\n",
      "loss at batch 300: 1.73\n",
      "loss at batch 400: 1.84\n",
      "Epoch 2\n",
      "loss at batch 0: 1.60\n",
      "loss at batch 100: 1.59\n",
      "loss at batch 200: 1.52\n",
      "loss at batch 300: 1.45\n",
      "loss at batch 400: 1.52\n",
      "Epoch 3\n",
      "loss at batch 0: 1.34\n",
      "loss at batch 100: 1.35\n",
      "loss at batch 200: 1.26\n",
      "loss at batch 300: 1.23\n",
      "loss at batch 400: 1.28\n",
      "Epoch 4\n",
      "loss at batch 0: 1.14\n",
      "loss at batch 100: 1.16\n",
      "loss at batch 200: 1.06\n",
      "loss at batch 300: 1.07\n",
      "loss at batch 400: 1.11\n",
      "Epoch 5\n",
      "loss at batch 0: 0.99\n",
      "loss at batch 100: 1.02\n",
      "loss at batch 200: 0.92\n",
      "loss at batch 300: 0.95\n",
      "loss at batch 400: 0.99\n",
      "Epoch 6\n",
      "loss at batch 0: 0.88\n",
      "loss at batch 100: 0.92\n",
      "loss at batch 200: 0.82\n",
      "loss at batch 300: 0.86\n",
      "loss at batch 400: 0.90\n",
      "Epoch 7\n",
      "loss at batch 0: 0.80\n",
      "loss at batch 100: 0.83\n",
      "loss at batch 200: 0.74\n",
      "loss at batch 300: 0.79\n",
      "loss at batch 400: 0.83\n",
      "Epoch 8\n",
      "loss at batch 0: 0.73\n",
      "loss at batch 100: 0.76\n",
      "loss at batch 200: 0.68\n",
      "loss at batch 300: 0.73\n",
      "loss at batch 400: 0.78\n",
      "Epoch 9\n",
      "loss at batch 0: 0.68\n",
      "loss at batch 100: 0.71\n",
      "loss at batch 200: 0.63\n",
      "loss at batch 300: 0.68\n",
      "loss at batch 400: 0.73\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "keras-intro.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
